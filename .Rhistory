#----Y-CatSal,X-Years
boxplot(myhit$Years~myhit$CatSal)
#Q-->C
#SUMMARIZATION
#VISUALIZATION
#----Y-CatSal,X-Years
boxplot(myhit$Years~myhit$CatSal,horizontal=TRUE)
#C-->C
#SUMMARIZATION
#Comparative Proportion
#VISUALIZATION
#Contingency Table
table(myhit$Division,myhit$CatSal)
#C-->C
#SUMMARIZATION
#Comparative Proportion
prop.table(table(myhit$Division,myhit$CatSal),1)
prop.table(table(myhit$League,myhit$CatSal),1)
#VISUALIZATION
#Contingency Table
chi.tab1<-table(myhit$Division,myhit$CatSal)
chi.tab2<-table(myhit$League,myhit$CatSal)
#TOH
#Chi-Square Test
chi.test1<-chisq.test(chi.tab1)
chi.test2<-chisq.test(chi.tab2)
chi.test1
chi.test2
#Q-->Q
#SUMMARIZATION
#Correlation Coefficient
#VISUALIZATION
#Scatter Plot
plot(myhit$Years~myhit$Salary)
#VISUALIZATION
#Scatter Plot
plot(myhit$Years,myhit$Salary)
#Q-->Q
#SUMMARIZATION
#Correlation Coefficient
cor(myhit$Years,myhit$Salary)
#Q-->Q
#SUMMARIZATION
#Correlation Coefficient
na.omit(myhit)
cor(myhit$Years,myhit$Salary)
#Q-->Q
#SUMMARIZATION
#Correlation Coefficient
myhit<-na.omit(myhit)
cor(myhit$Years,myhit$Salary)
#F value=79.56, p-value of F=6.11e-10
#Ha is accepted
#Milege of atleast 1 cly type is different from rest
#Based on EDA we can conclude that all are different
CatSal<-cut(myhit$Salary,breaks=3,labels = c("L","M","H"))
myhit<-data.frame(myhit,CatSal)
plot(myhit$HmRun,myhit$Salary)
data(mtcars)
cars<-data.frame(mtcars)
head(cars)
plot(cars$wt,cars$mpg,col=cars$cyl)
plot(cars$wt,cars$mpg,col=cars$am)
plot(cars$wt,cars$mpg,col=cars$cyl)
plot(cars$wt,cars$mpg,pch=cars$am)
plot(cars$wt,cars$mpg,pch=as.numeric(cars$am)+15)
plot(cars$wt,cars$mpg,pch=as.numeric(cars$am)+15, col=cars$cyl)
#Matrix Plot
pairs(myhit[,c(19,1:5)])
names(myhit)
#Matrix Plot
pairs(myhit[,c(19,15,1:3)])
cor(myhit[c(19,1:4)])
#Stratified Scatter Plot
#TOH
# t test for population slope
#y=beta0+beta1*x
#Ho:beat1=0;   Ha:beta1<>0
carslm1<-lm(mpg~wt,data=cars)
summary(carslm1)
#Ha Hypothesis is accepted,
#milege is dependent on wt
abline(carslm1)
#Stratified Scatter Plot
#TOH
# t test for population slope
#y=beta0+beta1*x
#Ho:beat1=0;   Ha:beta1<>0
carslm1<-lm(mpg~wt,data=cars)
#Ha Hypothesis is accepted,
#milege is dependent on wt
abline(carslm1)
plot(cars$wt,cars$mpg,col=cars$cyl)
#Ha Hypothesis is accepted,
#milege is dependent on wt
abline(carslm1)
plot(carslm1,3)
#MLR(1X,Many Y)(Y^=beta0+beta1*X1+beta2*X2...)
carslm2<-lm(mpg~wt+hp,data= cars)
#MLR(1X,Many Y)(Y^=beta0+beta1*X1+beta2*X2...)
plot(cars$hp,cars$mpg)
cor(cars$hp,cars$mpg)
#Measures
#TSS
#RSS
RSS2<-sum((carslm2$residuals)^2)
RSS2
#RSQ
(TSS-RSS2)/TSS
#Measures
#TSS(sum(y-ybar))
TSS<- sum((cars$mpg-mean(cars$mpg))^2)
TSS
#RSQ
(TSS-RSS2)/TSS
#Adj RSQ
#RSE
summary(carslm2)
plot(carslm2,1)
plot(carslm2,3)
predict(carslm2,newdata = data.frame(wt= c(2.5,3.5,4.5),hp=c(100,200,300)))
#Categorical(Dummy Variable)
#Polynomial(Y^=beta0+beta1*X1+beta2*X2)
carslm3a<-lm(mpg~hp,data= cars)#Only hp
carslm3b<-lm(mpg~hp+I(hp^2),data= cars)#hp^2
carslm3c<-lm(mpg~poly(hp,3),data= cars)#hp^3
RSS3a<-sum((carslm3a$residuals)^2)
RSS3b<-sum((carslm3b$residuals)^2)
RSS3c<-sum((carslm3c$residuals)^2)
RSS3a
RSS3b
RSS3c
summary(carslm3a)
summary(carslm3b)
summary(carslm3c)
plot(carslm3a,1)
plot(carslm3b,1)
plot(carslm3c,1)
plot(carslm3a,3)
plot(carslm3b,3)
plot(carslm3c,3)
abline(carslm3a)
plot(carslm3a,1)
abline(carslm3a)
plot(cars$hp,cars$mpg)
abline(carslm3a)
abline(carslm3b)
carslm3<-lm(mpg~wt+I(hp^2),data= cars)
#Measures
#TSS
#RSS
RSS3<-sum((carslm3$residuals)^2)
RSS3
#RSQ
(TSS-RSS3)/TSS
#Adj RSQ
#RSE
summary(carslm3)
carslm3<-lm(mpg~wt+hp+I(hp^2),data= cars)
#Measures
#TSS
#RSS
RSS3<-sum((carslm3$residuals)^2)
RSS3
#RSQ
(TSS-RSS3)/TSS
#Adj RSQ
#RSE
summary(carslm3)
carslm3<-lm(mpg~wt+hp+I(hp^2),data= cars)
#Measures
#TSS
#RSS
RSS3<-sum((carslm3$residuals)^2)
RSS3
#RSQ
(TSS-RSS3)/TSS
#Adj RSQ
#RSE
summary(carslm3)
plot(carslm3,1)
plot(carslm3,3)
lines(sort(cars$hp),fitted(carslm3b)[order(cars$hp)],col='red')
plot(carslm3b,1)
plot(carslm3c,1)
plot(carslm3a,3)
plot(carslm3b,3)
plot(carslm3c,3)
lines(sort(cars$hp),fitted(carslm3b)[order(cars$hp)],col='red')
plot(carslm3b,1)
lines(sort(cars$hp),fitted(carslm3b)[order(cars$hp)],col='red')
lines(sort(cars$hp),fitted(carslm3c)[order(cars$hp)],col='blue')
lines(sort(cars$hp),fitted(carslm3b)[order(cars$hp)],col='red')
lines(sort(cars$hp),fitted(carslm3c)[order(cars$hp)],col='blue')
plot(carslm3,1)
plot(carslm3,3)
par(mfrow=c(1,2))
plot(carslm3,1)
plot(carslm3,3)
#Categorical(Dummy Variable)
carslm4<-lm(mpg~wt+hp+I(hp^2)+cyl,data= cars)
#Measures
#TSS
#RSS
RSS4<-sum((carslm4$residuals)^2)
RSS4
#Adj RSQ
#RSE
summary(carslm4)
#Categorical(Dummy Variable)
carslm4a<-lm(cyl,data= cars)
#Categorical(Dummy Variable)
carslm4a<-lm(mpg~cyl,data= cars)
summary(carslm4a)
mean(mpg,cyl=4)
mean(cars$mpg,cars$cyl=4)
mean(cars$mpg,cars$cyl==4)
mean(cars[cars$cyl==4,c('mpg')])
vapply(cars$mpg,cars$cyl,mean)
tapply(cars$mpg,cars$cyl,mean)
predict(carslm4,newdata = data.frame(wt= c(2.5,3.5,4.5),hp=c(100,200,300)
,cyl=c(4,6,8)))
anova(carslm1,carslm2,carslm3,carslm4)
install.packages("stats")
install.packages("stats")
install.packages("stats")
install.packages("stats")
install.packages("stats")
install.packages("datasets.load")
install.packages("utilsIPEA")
install.packages("grDevices")
install.packages("grDevices")
install.packages("grDevices")
install.packages("grDevices")
install.packages("grDevices")
install.packages("graphicsQC")
library(dplyr)
library(ISLR)
library(graphicsQC)
library(MASS)
library(data.table)
library(datasets.load)
library(xlsx)
library(MASS, lib.loc = "C:/Program Files/R/R-3.6.0/library")
library(cluster, lib.loc = "C:/Program Files/R/R-3.6.0/library")
library(class, lib.loc = "C:/Program Files/R/R-3.6.0/library")
install.packages("tree")
install.packages("tree")
install.packages("leaps")
install.packages("glmnet")
install.packages("quantmod")
install.packages("forecast")
install.packages("fpp")
install.packages("boot")
install.packages("randomForest")
install.packages("e1071")
library(e1071)
library(fpp)
library(forecast)
library(glmnet)
library(tree)
library(xlsx)
library(randomForest)
library(quantmod)
install.packages("fpc")
install.packages("arules")
install.packages("arulesViz")
install.packages("neuralnet")
install.packages("xlsx")
install.packages("leaps")
install.packages("glmnet")
install.packages("quantmod")
install.packages("forecast")
install.packages("fpp")
install.packages("boot")
install.packages("classifierplots")
install.packages("tree")
install.packages("randomForest")
install.packages("e1071")
install.packages("cluster")
install.packages("fpc")
install.packages("arules")
install.packages("arulesViz")
grid1<-10^seq(10,-2,length.out = 100)
ridge.mod1<-glmnet(x.train,y.train,alpha = 1,lambda = grid1)#alpha =1 Lasso
#####------------Ridge AND Lasso
data("Hitters")
myhit<-data.frame(Hitters)
myhit<-na.omit(myhit)
library(glmnet)
library(Matrix)
library(foreach)
library(glmnet)
x<-model.matrix(Salary~.,data=myhit)
head(x)
x<-x[,-1]
head(x)
y<-myhit$Salary
grid1<-10^seq(10,-2,length.out = 100)
ridge.mod1<-glmnet(x.train,y.train,alpha = 1,lambda = grid1)#alpha =1 Lasso
set.seed(1)
train<-sample(1:nrow(x),0.65*nrow(x))
test<-(-train)
x.train<-x[train,]
x.test<-x[test,]
y.train<-y[train]
y.test<-y[test]
ridge.mod1<-glmnet(x.train,y.train,alpha = 1,lambda = grid1)#alpha =1 Lasso
plot(ridge.mod3)
ridge.mod3<-glmnet(x.train,y.train,alpha = 1,lambda = grid1)#alpha =1 Lasso
plot(ridge.mod3)
pred1=predict(ridge.mod3,newx=x.test)
err.df<-data.frame(lamb=0,err=0)
for(i in 1:dim(pred1)[2]){
err=sum((y.test-pred1[,i])^2)
err.df<-rbind(err.df,c(grid1[i],err))
}
print(err.df)
err.df<-err.df[-1,]
plot(err.df)
dim(err.df)
plot(log(err.df[,1],base = 10),err.df[2])
ridge.mod1<-glmnet(x.train,y.train,alpha = 0,lambda = grid1)#alpha=0 Ridge Regression
plot(ridge.mod)
plot(ridge.mod1)
err.df<-data.frame(lamb=0,err=0)
for(i in 1:dim(pred1)[2]){
err=sum((y.test-pred1[,i])^2)
err.df<-rbind(err.df,c(grid1[i],err))
}
print(err.df)
err.df<-err.df[-1,]
plot(err.df)
lasso.mod1<-glmnet(x.train,y.train,alpha = 1,lambda = 20)
beta20<-lasso.mod1$beta
coef(lasso.mod1)
lasso.mod2<-glmnet(x.train,y.train,alpha = 1,lambda = 2)
beta2<-lasso.mod1$beta
abs(beta20[,1])>abs(beta2[,1])
grid1<-10^seq(10,-2,length.out = 100)
grid2<-c(0,0.01,0.05,0.1,0.2,0.5,1,2,3,5,10,15,20,30,40,50,80,100,150,200,250,300,350
,400,450,500,600,700,800,900,1000,1500,2000)
lasso.mod1<-glmnet(x.train,y.train,alpha = 1,lambda = grid1)#alpha=1 Lasso Regression
lasso.mod2<-glmnet(x.train,y.train,alpha = 1,lambda = grid2)
colnames(x)
dim(coef(lasso.mod1))
plot(lasso.mod1)
plot(lasso.mod2)
plot(grid,coef(lasso.mod1)[12,])
pred1=predict(lasso.mod1,newx=x.test)
pred2=predict(lasso.mod2,newx=x.test)
err.df<-data.frame(lamb=0,err=0)
for(i in 1:dim(pred1)[2]){
err=sum((y.test-pred1[,i])^2)
err.df<-rbind(err.df,c(grid1[i],err))
}
print(err.df)
err.df<-err.df[-1,]
plot(err.df[50:100,])
dim(err.df)
plot(log(err.df[,1],base = 10),err.df[2])
min(err.df[,2])#min error
err.df[100,]
err=sum(y.test)
plot(log(err.df[,1],base = 10),err.df[2])
plot(log(err.df[,1],base = 10),err.df[,2])
set.seed(1)
cv.out=cv.glmnet(x[train,],y[train,],alpha=0,lambda = grid1)
plot(cv.out)
cv.out=cv.glmnet(x[train,],y[train],alpha=0,lambda = grid1)
plot(cv.out)
bestlm=cv.out$lambda.min #best value of lambda
bestlm
set.seed(1)
cv.out1=cv.glmnet(x[train,],y[train],alpha=1,lambda = grid1)
plot(cv.out1)
bestlm=cv.out1$lambda.min #best value of lambda
bestlm
x.train<-x[train,]
x.train
x.train<-x[train,]
#########----------DECISION TREE
library(tree)
HighSal=ifelse(myhit$Salary<1000,"No","Yes")
myhit=data.frame(myhit,HighSal)
head(myhit)
#Regression Tree
my.reg.tree<-tree(salary~Hits+Years,data=myhit)
#########----------DECISION TREE
library(tree)
#########----------DECISION TREE
install.packages(tree)
#########----------DECISION TREE
library(tree)
library(randomForest)
#Regression Tree
my.reg.tree<-tree(salary~Hits+Years,data=myhit)
lasso.mod1<-glmnet(x.train,y.train,alpha=1,lambda = bestlm)
sum(coef(lasso.mod1)[,1]!=0)#count of non zero coefficients in lasso
pred=predict(lasso.mod1,newx=x.test)
knitr::opts_chunk$set(echo = TRUE)
summary(cars)
plot(pressure)
install.packages("tree")
library(stats)#---For Data Exploration
library(dplyr)#---For Data Manipulation
getwd()
setwd("c:\\project")
getwd()
mydata<-read.csv("Attrition Case Study.csv")#Data is Loaded into mydata object from csv file
head(mydata)
str(mydata)
sum(is.na(mydata))
summary(mydata)
####--------DistanceFromHome
boxplot(mydata$DistanceFromHome~mydata$Attrition,col=c(2,4))$stats
#anova F test
summary(aov(mydata$DistanceFromHome~mydata$Attrition))
t<-table(mydata$DistanceFromHome~mydata$Attrition)
t<-table(mydata$DistanceFromHome,mydata$Attrition)
barplot(t,col = c(2,4),beside = FALSE)
barplot(t,col = c(2,4),beside = TRUE)
####--------DistanceFromHome
boxplot(mydata$DistanceFromHome~mydata$Attrition,col=c(2,4))$stats
####--------DistanceFromHome
boxplot(mydata$DistanceFromHome~mydata$Attrition,col=c(2,4))
####--------DistanceFromHome
boxplot(mydata$DistanceFromHome~mydata$Attrition,col=c(2,4))
#t<-table(mydata$DistanceFromHome,mydata$Attrition)
#barplot(t,col = c(2,4),beside = TRUE)
####--------MonthlyIncome
boxplot(mydata$MonthlyIncome~mydata$Attrition,col=c(2,4))$stats
##-----MonthlyRate
boxplot(mydata$MonthlyRate~mydata$Attrition,col=c(2,4))$stats
####-------TotalWorkingYears
boxplot(mydata$TotalWorkingYears~mydata$Attrition,col=c(2,4))$stats
####-------YearsAtCompany
boxplot(mydata$YearsAtCompany~mydata$Attrition,col=c(2,4))$stats
####-------YearsInCurrentRole
boxplot(mydata$YearsInCurrentRole~mydata$Attrition,col=c(2,4))$stats
####--------YearsWithCurrManager
boxplot(mydata$YearsWithCurrManager~mydata$Attrition,col=c(2,4))$stats
####-------Attrition rate with BusinessTravel
mytable<-xtabs(~BusinessTravel + Attrition ,data=mydata)
prop.table(mytable,1)
#VISUALIZATION
#Contingency Table
chi.tab1<-table(mydata$BusinessTravel,mydata$Attrition)
mosaic(mytable, main = "mydata")
#C-->C
#SUMMARIZATION
#Comparative Proportion
library(vcd)
mosaic(mytable, main = "mydata")
mosaic(mytable,col=c(1,2), main = "mydata")
mosaic(mytable,col=c("Red","Green"), main = "mydata")
?mosaic
mosaic(mytable,highlighting_fill = red.colors, main = "mydata")
?mosaic
mosaic(mytable, main = "mydata")
mosaic(mytable,shade = TRUE, main = "mydata")
library(ggplot2)
ggplot(mydata,aes(Attrition,
BusinessTravel)) +
geom_bar(position = "stack")
####--------Department wise Attrition Rate
mytable<-xtabs(~Department + Attrition ,data=mydata)
mosaic(mytable,shade = TRUE, main = "mydata")
##------Educationwise Attrition Rate
mytable<-xtabs(~Education + Attrition ,data=mydata)
mosaic(mytable,shade = TRUE, main = "mydata")
##------Educationwise Attrition Rate
mytable<-xtabs(~Education + Attrition ,data=mydata)
mosaic(mytable,shade = TRUE, main = "mydata")
mosaic(mytable, main = "mydata")
mosaic(mytable,shade = TRUE, main = "mydata")
####----------EducationFieldwise Attrition Rate
mytable<-xtabs(~EducationField + Attrition ,data=mydata)
mosaic(mytable,shade = TRUE, main = "mydata")
####----------Effect of EnvironmentSatisfaction on Attrition
mytable<-xtabs(~EnvironmentSatisfaction + Attrition ,data=mydata)
mosaic(mytable,shade = TRUE, main = "mydata")
##---Attrition Rate with Gender
mytable<-xtabs(~Gender + Attrition ,data=mydata)
mosaic(mytable,shade = TRUE, main = "mydata")
####--------Attrition Rate with JobInvolvement
mytable<-xtabs(~JobInvolvement + Attrition ,data=mydata)
mosaic(mytable,shade = TRUE, main = "mydata")
####--------Effect of JobLevel on Attrition rate
mytable<-xtabs(~JobLevel + Attrition ,data=mydata)
mosaic(mytable,shade = TRUE, main = "mydata")
####--------Effect of JobRole on Attrition rate
mytable<-xtabs(~JobRole + Attrition ,data=mydata)
mosaic(mytable,shade = TRUE, main = "mydata")
####--------Effect of JobSatisfaction on Attrition rate
mytable<-xtabs(~JobSatisfaction + Attrition ,data=mydata)
mosaic(mytable,shade = TRUE, main = "mydata")
####---------Effect of MaritalStatus on Attrition rate
mytable<-xtabs(~MaritalStatus + Attrition ,data=mydata)
mosaic(mytable,shade = TRUE, main = "mydata")
####---------Effect of OverTime on Attrition rate
mytable<-xtabs(~OverTime + Attrition ,data=mydata)
mosaic(mytable,shade = TRUE, main = "mydata")
##--Effect of PerformanceRating on Attrition rate
mytable<-xtabs(~PerformanceRating + Attrition ,data=mydata)
mosaic(mytable,shade = TRUE, main = "mydata")
####---------Effect of RelationSatisfaction on Attrition rate
mytable<-xtabs(~RelationshipSatisfaction + Attrition ,data=mydata)
mosaic(mytable,shade = TRUE, main = "mydata")
####---------Effect of WorkLifeBalance on Attrition rate
mytable<-xtabs(~WorkLifeBalance + Attrition ,data=mydata)
mosaic(mytable,shade = TRUE, main = "mydata")
